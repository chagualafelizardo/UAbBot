from rasa_sdk import Action
from collections import Counter
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk import pos_tag
import pymongo
import logging
import re
from typing import Dict, List, Any, Optional, Tuple
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from transformers import RobertaTokenizer, RobertaModel
import torch
from datetime import datetime
from collections import defaultdict
import string
import nltk


logger = logging.getLogger(__name__)

class ActionSmartSearch(Action):
    def name(self):
        return "action_search_documents"

    def __init__(self):
        super().__init__()
        self.logger = logging.getLogger(__name__)
        self.min_similarity = 0.4
        self.faq_threshold = 0.82
        self.top_k = 5  # Aumentado para capturar mais resultados
        self.context_window = 3
        self.financial_threshold = 0.75  # Limiar espec√≠fico para quest√µes financeiras
        
        # Conex√£o com MongoDB
        self.client = pymongo.MongoClient(
            "mongodb://root:root@uabbot-mongodb-1:27017/",
            serverSelectionTimeoutMS=5000,
            socketTimeoutMS=30000,
            connectTimeoutMS=30000,
            retryWrites=True
        )
        self.db = self.client["uab"]
        self.collection = self.db["documents"]
        
        # Modelos de embeddings
        try:
            self.sentence_model = SentenceTransformer(
                'paraphrase-multilingual-MiniLM-L12-v2',
                device='cpu'
            )
            self.roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
            self.roberta_model = RobertaModel.from_pretrained('roberta-base').eval()
        except Exception as e:
            self.logger.error(f"Erro ao carregar modelos: {str(e)}")
            raise

        # Configura√ß√µes RAG
        self._initialize_rag_environment()

    def _initialize_rag_environment(self):
        """Inicializa o ambiente RAG com √≠ndices e cole√ß√µes necess√°rias"""
        try:
            # Verificar conex√£o
            self.client.admin.command('ping')
            
            # Criar cole√ß√£o para embeddings se n√£o existir
            if "document_embeddings" not in self.db.list_collection_names():
                self.db.create_collection("document_embeddings")
                self.logger.info("Cole√ß√£o document_embeddings criada")
            
            # Criar √≠ndices de texto
            if "text_content_text" not in self.collection.index_information():
                self.collection.create_index(
                    [("text_content", "text")],
                    default_language="portuguese"
                )
            
            # Verificar e completar embeddings ausentes
            self._verify_and_complete_embeddings()
            
        except Exception as e:
            self.logger.error(f"Erro na inicializa√ß√£o do RAG: {str(e)}")
            raise

    def _verify_and_complete_embeddings(self):
        """Verifica e completa embeddings ausentes"""
        try:
            total_docs = self.collection.count_documents({"text_content": {"$exists": True, "$ne": ""}})
            total_embeddings = self.db.document_embeddings.count_documents({})
            
            self.logger.info(f"Documentos: {total_docs}, Embeddings: {total_embeddings}")
            
            if total_docs > total_embeddings:
                self.logger.warning("Alguns documentos est√£o sem embeddings. Gerando...")
                self._generate_missing_embeddings()
        except Exception as e:
            self.logger.error(f"Erro ao verificar embeddings: {str(e)}")

    def _generate_missing_embeddings(self):
        """Gera embeddings para documentos ausentes"""
        try:
            missing_docs = list(self.collection.aggregate([
                {
                    "$lookup": {
                        "from": "document_embeddings",
                        "localField": "_id",
                        "foreignField": "doc_id",
                        "as": "embeddings"
                    }
                },
                {
                    "$match": {
                        "embeddings": {"$size": 0},
                        "text_content": {"$exists": True, "$ne": ""}
                    }
                },
                {
                    "$limit": 100  # Limite para n√£o sobrecarregar
                }
            ]))

            if not missing_docs:
                self.logger.info("Todos os documentos possuem embeddings")
                return

            self.logger.info(f"Encontrados {len(missing_docs)} documentos sem embeddings")

            for doc in missing_docs:
                try:
                    content = doc.get("text_content", "")
                    if not content:
                        continue

                    embedding = self._get_roberta_embedding(content)
                    
                    self.db.document_embeddings.insert_one({
                        "doc_id": doc["_id"],
                        "filename": doc.get("filename", ""),
                        "embedding": embedding.tolist(),
                        "last_updated": datetime.utcnow(),
                        "content_preview": content[:200] + "..." if len(content) > 200 else content
                    })
                    
                except Exception as e:
                    self.logger.error(f"Erro ao gerar embedding para {doc.get('filename')}: {str(e)}")
                    continue

        except Exception as e:
            self.logger.error(f"Falha ao gerar embeddings ausentes: {str(e)}")

    def _get_roberta_embedding(self, text: str) -> np.ndarray:
        """Gera embeddings com normaliza√ß√£o"""
        try:
            inputs = self.roberta_tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding='max_length'
            )
            with torch.no_grad():
                outputs = self.roberta_model(**inputs)
            embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze().numpy()
            return embedding / np.linalg.norm(embedding)
        except Exception as e:
            self.logger.error(f"Erro ao gerar embedding: {str(e)}")
            raise

    def _semantic_search_rag(self, query: str, top_k: int = 5) -> List[Dict]:
        """Busca sem√¢ntica otimizada com pr√©-filtro por texto"""
        try:
            # Pr√©-filtro por texto para melhor performance
            text_ids = [doc['_id'] for doc in self.collection.find(
                {"$text": {"$search": query}},
                {"_id": 1}
            ).limit(100)]
            
            query_embedding = self._get_roberta_embedding(query)
            
            pipeline = [
                {"$match": {"doc_id": {"$in": text_ids}}} if text_ids else {"$match": {}},
                {
                    "$addFields": {
                        "similarity": {
                            "$let": {
                                "vars": {
                                    "dot_product": {
                                        "$reduce": {
                                            "input": {"$zip": {"inputs": ["$embedding", query_embedding.tolist()]}},
                                            "initialValue": 0,
                                            "in": {
                                                "$add": [
                                                    "$$value",
                                                    {"$multiply": [
                                                        {"$arrayElemAt": ["$$this", 0]},
                                                        {"$arrayElemAt": ["$$this", 1]}
                                                    ]}
                                                ]
                                            }
                                        }
                                    },
                                    "query_norm": {
                                        "$sqrt": {
                                            "$reduce": {
                                                "input": query_embedding.tolist(),
                                                "initialValue": 0,
                                                "in": {"$add": ["$$value", {"$pow": ["$$this", 2]}]}
                                            }
                                        }
                                    },
                                    "embedding_norm": {
                                        "$sqrt": {
                                            "$reduce": {
                                                "input": "$embedding",
                                                "initialValue": 0,
                                                "in": {"$add": ["$$value", {"$pow": ["$$this", 2]}]}
                                            }
                                        }
                                    }
                                },
                                "in": {
                                    "$divide": [
                                        "$$dot_product",
                                        {"$multiply": ["$$query_norm", "$$embedding_norm"]}
                                    ]
                                }
                            }
                        }
                    }
                },
                {"$sort": {"similarity": -1}},
                {"$limit": top_k},
                {
                    "$lookup": {
                        "from": "documents",
                        "localField": "doc_id",
                        "foreignField": "_id",
                        "as": "document"
                    }
                },
                {"$unwind": "$document"},
                {
                    "$project": {
                        "doc_id": 1,
                        "similarity": 1,
                        "filename": "$document.filename",
                        "content": "$document.text_content",
                        "content_preview": 1
                    }
                }
            ]
            
            return list(self.db.document_embeddings.aggregate(pipeline))
            
        except Exception as e:
            self.logger.error(f"Erro na busca sem√¢ntica: {str(e)}", exc_info=True)
            return []

    def _preprocess_text(self, text: str) -> str:
        """Pr√©-processamento melhorado mantendo pontua√ß√£o relevante"""
        text = text.lower()
        text = re.sub(r'[^\w\s.,!?]', '', text)  # Mant√©m pontua√ß√£o b√°sica
        return text.strip()

    def _is_financial_query(self, query: str) -> bool:
        """Identifica se a consulta √© sobre quest√µes financeiras"""
        financial_keywords = [
            "propina", "propinas", "pagamento", "pagar", "financiamento",
            "bolsa", "aux√≠lio", "parcelamento", "d√≠vida", "divida", "valor",
            "pre√ßo", "preco", "custos", "isen√ß√£o", "isencao", "mensalidade"
        ]
        return any(keyword in self._preprocess_text(query) for keyword in financial_keywords)

    def _extract_financial_info(self, content: str) -> Dict:
        """Extrai informa√ß√µes financeiras espec√≠ficas do conte√∫do"""
        financial_data = {
            "payment_options": [],
            "scholarships": [],
            "exemptions": [],
            "contacts": []
        }
        
        # Extrai op√ß√µes de pagamento
        payment_section = re.search(r'(Op√ß√µes de Pagamento|Formas de Pagamento)[\s\S]*?(?=\n\s*\n)', content, re.IGNORECASE)
        if payment_section:
            payments = re.findall(r'‚Ä¢\s*([^\n]+)|-\s*([^\n]+)', payment_section.group(0))
            financial_data["payment_options"] = [p[0] or p[1] for p in payments if p[0] or p[1]]
        
        # Extrai bolsas e aux√≠lios
        scholarship_section = re.search(r'(Bolsas|Aux√≠lios|Financiamentos)[\s\S]*?(?=\n\s*\n)', content, re.IGNORECASE)
        if scholarship_section:
            scholarships = re.findall(r'‚Ä¢\s*([^\n]+)|-\s*([^\n]+)', scholarship_section.group(0))
            financial_data["scholarships"] = [s[0] or s[1] for s in scholarships if s[0] or s[1]]
        
        # Extrai contatos
        contact_section = re.search(r'(Contatos|Contactos)[\s\S]*?(?=\n\s*\n)', content, re.IGNORECASE)
        if contact_section:
            contacts = re.findall(r'‚Ä¢\s*([^\n]+)|-\s*([^\n]+)', contact_section.group(0))
            financial_data["contacts"] = [c[0] or c[1] for c in contacts if c[0] or c[1]]
        
        return financial_data

    def _extract_faqs(self, content: str) -> List[Dict]:
        """Extrai FAQs formatadas do conte√∫do com separa√ß√£o precisa por perguntas numeradas"""
        faqs = []
        
        # Divide o conte√∫do em blocos de perguntas/respostas numeradas
        # Padr√£o para capturar: n√∫mero, pergunta e resposta at√© a pr√≥xima pergunta numerada
        faq_blocks = re.findall(
            r'(?:^|\n)(\d+\.)\s*([^\n?]+\??)\s*([^\n]+(?:\n(?!\d+\.\s)[^\n]*)*)', 
            content, 
            re.MULTILINE
        )
        
        for block in faq_blocks:
            number = block[0].strip()
            question = block[1].strip()
            answer = block[2].strip() if block[2] else ""
            
            # Limpa a resposta removendo espa√ßos extras
            answer = re.sub(r'\s+', ' ', answer).strip()
            
            # Verifica se temos conte√∫do v√°lido
            if len(question.split()) >= 3 and len(answer.split()) >= 5:
                faqs.append({
                    'number': number,
                    'question': question,
                    'answer': answer,
                    'full_text': f"{number} {question}\n{answer}",
                    'is_financial': any(fin_word in question.lower() for fin_word in ['propina', 'pagamento', 'bolsa'])
                })
        
        return faqs

    def _is_faq_query(self, query: str) -> bool:
        """Determina se a consulta parece uma pergunta de FAQ"""
        question_words = [
            "como", "qual", "quando", "onde", "por que", "quais", "quanto", 
            "contato", "telefone", "email", "endere√ßo", "pode", "deve", "existe",
            "posso", "preciso", "dificuldade", "problema", "ajuda", "d√∫vida",
            "propina", "propinas", "pagamento", "pagar", "financiamento", "bolsa",
            "conciliar", "concilia√ß√£o", "equilibrar", "vida profissional", "tempo"
        ]
        
        query_lower = self._preprocess_text(query)
        
        return (
            any(word in query_lower for word in question_words) or
            "?" in query or
            any(phrase in query_lower for phrase in [" o que ", " em que ", " para que "]) or
            re.search(r'\b(pode|deve|como)\s+[^\s]+\s+', query_lower) is not None
        )

    def _find_best_faq_match(self, query: str, docs: List[Dict]) -> Optional[Dict]:
        """Encontra a melhor correspond√™ncia de FAQ nos documentos com foco em perguntas numeradas"""
        query_embedding = self.sentence_model.encode(query)
        best_match = None
        highest_score = 0
        
        for doc in docs:
            content = doc.get("content", doc.get("text_content", ""))
            if not content:
                continue
                
            faqs = self._extract_faqs(content)
            if not faqs:
                continue
                
            for faq in faqs:
                # Para perguntas numeradas, verifica correspond√™ncia exata com o n√∫mero
                if re.match(r'^\d+\.', faq['question']):
                    # Extrai o n√∫mero da pergunta (ex: "5.")
                    question_number = faq['number']
                    # Verifica se o n√∫mero est√° na query (ex: query cont√©m "5.")
                    if question_number in query:
                        return {
                            'type': 'faq',
                            'question': faq['question'],
                            'answer': faq['answer'],
                            'score': 1.0,  # M√°xima similaridade
                            'filename': doc.get("filename", "documento"),
                            'content': content,
                            'is_financial': faq.get('is_financial', False)
                        }
                    
                # Caso contr√°rio, usa similaridade sem√¢ntica
                clean_question = self._preprocess_text(faq['question'])
                clean_answer = self._preprocess_text(faq['answer'])
                clean_query = self._preprocess_text(query)
                
                question_emb = self.sentence_model.encode(clean_question)
                answer_emb = self.sentence_model.encode(clean_answer)
                
                question_sim = cosine_similarity([query_embedding], [question_emb])[0][0]
                answer_sim = cosine_similarity([query_embedding], [answer_emb])[0][0]
                similarity = max(question_sim, answer_sim)
                
                # B√¥nus para FAQs financeiras se a pergunta for financeira
                if self._is_financial_query(query) and faq.get('is_financial', False):
                    similarity = min(similarity + 0.15, 1.0)
                    
                if similarity > highest_score and similarity >= (self.faq_threshold if not self._is_financial_query(query) else self.financial_threshold):
                    highest_score = similarity
                    best_match = {
                        'type': 'faq',
                        'question': faq['question'],
                        'answer': faq['answer'],
                        'score': similarity,
                        'filename': doc.get("filename", "documento"),
                        'content': content,
                        'is_financial': faq.get('is_financial', False)
                    }
        
        return best_match

    def _find_relevant_section(self, content: str, query: str) -> Dict:
        """Encontra a se√ß√£o mais relevante no conte√∫do"""
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        best_paragraph = ""
        best_score = 0
        
        query_embedding = self.sentence_model.encode(query)
        is_financial = self._is_financial_query(query)
        
        for para in paragraphs:
            if len(para.split()) < 5:
                continue
                
            para_embedding = self.sentence_model.encode(para)
            similarity = cosine_similarity([query_embedding], [para_embedding])[0][0]
            
            # B√¥nus para par√°grafos financeiros se a pergunta for financeira
            if is_financial and any(fin_word in para.lower() for fin_word in ['propina', 'pagamento', 'bolsa']):
                similarity = min(similarity + 0.15, 1.0)
            
            if similarity > best_score:
                best_score = similarity
                best_paragraph = para
        
        if best_score > 0.5:
            return {'text': best_paragraph, 'score': best_score}
        
        # Fallback: retorna o in√≠cio do conte√∫do
        return {'text': content[:500], 'score': 0.4}

    def _format_financial_response(self, result: Dict) -> List[Dict]:
        """Formata resposta especializada para quest√µes financeiras"""
        financial_info = self._extract_financial_info(result['content'])
        
        response_parts = [
            {
                'text': f"üí≥ **Informa√ß√µes sobre pagamentos ({result['filename']}):**",
                'metadata': {"type_speed": 20}
            }
        ]
        
        # Adiciona op√ß√µes de pagamento
        if financial_info["payment_options"]:
            response_parts.append({
                'text': "\nüìã **Op√ß√µes de pagamento dispon√≠veis:**\n‚Ä¢ " + "\n‚Ä¢ ".join(financial_info["payment_options"][:5]),
                'metadata': {"type_speed": 15}
            })
        
        # Adiciona bolsas e aux√≠lios
        if financial_info["scholarships"]:
            response_parts.append({
                'text': "\nüéì **Bolsas e aux√≠lios:**\n‚Ä¢ " + "\n‚Ä¢ ".join(financial_info["scholarships"][:3]),
                'metadata': {"type_speed": 15}
            })
        
        # Adiciona contatos
        if financial_info["contacts"]:
            response_parts.append({
                'text': "\nüìû **Contatos √∫teis:**\n‚Ä¢ " + "\n‚Ä¢ ".join(financial_info["contacts"][:3]),
                'metadata': {"type_speed": 15}
            })
        
        # Rodap√© com a√ß√µes
        response_parts.append({
            'text': "\nüîç Para mais detalhes ou solicitar condi√ß√µes especiais, entre em contato com o servi√ßo financeiro.",
            'metadata': {
                'type_speed': 30,
                'buttons': [
                    {
                        'title': 'üìû Contato Financeiro',
                        'payload': '/contato_financeiro'
                    },
                    {
                        'title': 'üìÑ Regulamento Completo',
                        'payload': '/regulamento_propinas'
                    }
                ]
            }
        })
        
        return response_parts

    def _generate_related_questions(self, faq_match: Dict) -> List[str]:
        """Gera perguntas relevantes baseadas em an√°lise sem√¢ntica do conte√∫do"""
        try:
            answer = faq_match['answer']
            question = faq_match['question']
            
            # Primeiro, identificamos os t√≥picos principais na resposta
            main_topics = self._extract_main_topics(answer)
            
            # Depois identificamos conceitos espec√≠ficos
            specific_concepts = self._extract_specific_concepts(answer)
            
            # Finalmente detectamos a√ß√µes/recomenda√ß√µes
            actions_recommendations = self._extract_actions_recommendations(answer)
            
            # Gera√ß√£o de perguntas inteligentes
            generated_questions = []
            
            # 1. Perguntas sobre t√≥picos principais
            for topic in main_topics[:2]:
                if len(topic.split()) <= 4:  # Evita t√≥picos muito longos
                    generated_questions.append(f"Como {topic} afeta meu desempenho acad√™mico?")
                    generated_questions.append(f"Quais s√£o as melhores estrat√©gias para lidar com {topic}?")
            
            # 2. Perguntas sobre conceitos espec√≠ficos
            for concept in specific_concepts:
                if concept.lower() not in question.lower():  # Evita repeti√ß√£o
                    generated_questions.append(f"Como funciona {concept} na UAb?")
                    generated_questions.append(f"Quem pode me ajudar com quest√µes sobre {concept}?")
            
            # 3. Perguntas sobre a√ß√µes/recomenda√ß√µes
            for action in actions_recommendations:
                if "inscri" in action:
                    generated_questions.append("Como fa√ßo para me inscrever nessa op√ß√£o?")
                elif "contact" in action or "falar" in action:
                    generated_questions.append("Qual √© o contato para essa assist√™ncia?")
                elif "estrat√©gia" in action or "m√©todo" in action:
                    generated_questions.append("Onde posso aprender mais sobre essas estrat√©gias?")
            
            # Filtro de qualidade
            final_questions = []
            seen = set()
            for q in generated_questions:
                clean_q = q.lower().replace("?", "").strip()
                if (clean_q not in seen and 
                    len(q.split()) > 5 and 
                    not any(word in q.lower() for word in ["qual √© a fun√ß√£o", "como o"])):
                    seen.add(clean_q)
                    final_questions.append(q)
            
            return final_questions[:3] if final_questions else []

        except Exception as e:
            self.logger.error(f"Erro na gera√ß√£o de perguntas: {str(e)}")
            return []

    def _extract_main_topics(self, text: str) -> List[str]:
        """Identifica os t√≥picos principais mencionados no texto"""
        # Padr√£o para frases importantes (normalmente ap√≥s dois pontos ou marcadores)
        topics = re.findall(r'(?:^|\n|‚Ä¢\s)([A-Z][^.:?!]+?)(?=[.:?!]|\n|$)', text)
        
        # Filtra t√≥picos por relev√¢ncia
        filtered = []
        common_words = {"isso", "que", "qual", "como", "quando"}
        for topic in topics:
            words = topic.split()
            if (4 <= len(words) <= 8 and 
                not any(w.lower() in common_words for w in words[:3])):
                filtered.append(topic.strip())
        
        return list(dict.fromkeys(filtered))[:5]  # Remove duplicados e limita

    def _extract_specific_concepts(self, text: str) -> List[str]:
        """Extrai conceitos espec√≠ficos (siglas, nomes pr√≥prios, termos t√©cnicos)"""
        # Padr√£o para siglas (2-4 letras mai√∫sculas)
        acronyms = re.findall(r'\b[A-Z]{2,4}\b', text)
        
        # Padr√£o para termos t√©cnicos entre aspas ou em it√°lico
        terms = re.findall(r'"(.*?)"|\'(.*?)\'|\b([A-Z][a-z]+ [A-Z][a-z]+)\b', text)
        flat_terms = [t for group in terms for t in group if t]
        
        # Padr√£o para nomes de servi√ßos/departamentos
        services = re.findall(r'\b([A-Z][a-z]+ (?:de|do|da) [A-Z][a-z]+)\b', text)
        
        # Combina todos os conceitos
        all_concepts = acronyms + flat_terms + services
        return list(dict.fromkeys(all_concepts))[:5]  # Remove duplicados e limita

    def _extract_actions_recommendations(self, text: str) -> List[str]:
        """Identifica a√ß√µes ou recomenda√ß√µes espec√≠ficas no texto"""
        # Padr√£o para verbos no infinitivo seguidos de complemento
        actions = re.findall(r'\b([Pp]ode [a-z√°√©√≠√≥√∫√¢√™√¥√ß√£√µ]+)\b|\b([Dd]eve [a-z√°√©√≠√≥√∫√¢√™√¥√ß√£√µ]+)\b', text)
        flat_actions = [a for group in actions for a in group if a]
        
        # Padr√£o para recomenda√ß√µes expl√≠citas
        recommendations = re.findall(r'\b([Rr]ecomenda-se [^.!?]+)|([Cc]ontate [^.!?]+)', text)
        flat_recommendations = [r for group in recommendations for r in group if r]
        
        return (flat_actions + flat_recommendations)[:5]  # Limita a 5 itens

    def _extract_context_keywords(self, text: str) -> Dict[str, List[str]]:
        """Extrai palavras-chave contextuais sem usar NLTK"""
        keywords = {
            "prazo": [],
            "documento": [],
            "contato": [],
            "requisito": []
        }
        
        # Mapeamento de padr√µes
        patterns = {
            "prazo": r"(prazo|at√©|dia|data|vencimento).{0,20}?\b(\d{1,2}\/\d{1,2}\/\d{2,4}|\d+\s+dias?)\b",
            "documento": r"(documento|formul√°rio|requerimento|comprovante).{0,20}?(entregar|apresentar|necess√°rio)",
            "contato": r"(contatar|telefone|email|setor|secretaria).{0,20}?\b(\+?\d{9,15}|\b[\w\.-]+@[\w\.-]+\.\w{2,}\b)",
            "requisito": r"(requer|necess√°rio|precisa).{0,15}?(ter|possuir|estar)"
        }
        
        for key, pattern in patterns.items():
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                if match.group():
                    keywords[key].append(match.group())
        
        return keywords

    def _extract_entities(self, text: str) -> List[str]:
        """Extrai entidades importantes sem usar NLTK"""
        # Padr√µes para entidades (mai√∫sculas ap√≥s pontua√ß√£o ou no in√≠cio)
        entities = re.findall(r"(?:^|[.!?]\s+)([A-Z][a-z√°√©√≠√≥√∫√¢√™√¥√ß√£√µ]+(?:\s+[A-Z][a-z√°√©√≠√≥√∫√¢√™√¥√ß√£√µ]+)*)", text)
        
        # Filtra entidades comuns e repetidas
        common_entities = {"Universidade", "Aluno", "Curso", "Processo"}
        unique_entities = []
        seen = set()
        
        for entity in entities:
            clean_entity = entity.strip()
            if (clean_entity not in seen and 
                clean_entity not in common_entities and
                len(clean_entity.split()) < 4):
                seen.add(clean_entity)
                unique_entities.append(clean_entity)
        
        return unique_entities[:5]  # Limita a 5 entidades

    def _extract_actions(self, text: str) -> List[Tuple[str, str]]:
        """Extrai pares a√ß√£o-alvo sem usar NLTK"""
        # Padr√£o: verbo + substantivo
        actions = []
        matches = re.finditer(r"\b([a-z√°√©√≠√≥√∫√¢√™√¥√ß√£√µ]+r)\b.{0,10}?\b([a-z√°√©√≠√≥√∫√¢√™√¥√ß√£√µ]{3,}s?\b)", text, re.IGNORECASE)
        
        for match in matches:
            verb = match.group(1).lower()
            noun = match.group(2).lower()
            
            # Filtra verbos comuns
            if verb not in {"ser", "ter", "haver", "poder"}:
                actions.append((verb, noun))
        
        return actions[:5]  # Limita a 5 a√ß√µes

    def _format_faq_response(self, faq_match: Dict) -> List[Dict]:
        """Formata resposta para FAQ com perguntas contextualizadas"""
        response_parts = [
            {
                'text': f"‚ùì **Pergunta encontrada em {faq_match['filename']}:**\n{faq_match['question']}",
                'metadata': {
                    'response_part': 'question',
                    'complete_before_next': True
                }
            },
            {
                'text': f"‚úÖ **Resposta completa:**\n{faq_match['answer']}",
                'metadata': {
                    'response_part': 'answer',
                    'complete_before_next': True
                }
            }
        ]
        
        # Gera perguntas contextualizadas
        related_questions = self._generate_related_questions(faq_match)
        
        if related_questions:
            questions_text = "\n\nüîç Com base nesta informa√ß√£o, voc√™ pode querer saber:"
            response_parts.append({
                'text': questions_text,
                'metadata': {
                    'type_speed': 20,
                    'suggested_questions': related_questions
                }
            })
        else:
            # Se n√£o gerou perguntas, oferece ajuda gen√©rica
            response_parts.append({
                'text': "\n\nPosso te ajudar com mais alguma informa√ß√£o sobre este assunto?",
                'metadata': {
                    'type_speed': 20
                }
            })
        
        # Feedback
        response_parts.append({
            'text': "\nEsta informa√ß√£o foi √∫til?",
            'metadata': {
                'buttons': [
                    {'title': 'üëç Sim', 'payload': '/feedback_positive'},
                    {'title': 'üëé N√£o', 'payload': '/feedback_negative'}
                ],
                'complete_before_next': True
            }
        })
        
        return response_parts

    def _format_faq_response(self, faq_match: Dict) -> List[Dict]:
        """Formata resposta para FAQ com perguntas sugeridas geradas dinamicamente"""
        response_parts = [
            {
                'text': f"‚ùì **Pergunta encontrada em {faq_match['filename']}:**\n{faq_match['question']}",
                'metadata': {
                    'response_part': 'question',
                    'complete_before_next': True
                }
            },
            {
                'text': f"‚úÖ **Resposta completa:**\n{faq_match['answer']}",
                'metadata': {
                    'response_part': 'answer',
                    'complete_before_next': True
                }
            }
        ]
        
        # Gera perguntas relacionadas de forma inteligente
        related_questions = self._generate_related_questions(faq_match)
        
        if related_questions:
            questions_text = "\n\nüîç Talvez voc√™ queira saber tamb√©m sobre:"
            response_parts.append({
                'text': questions_text,
                'metadata': {
                    'type_speed': 20,
                    'suggested_questions': related_questions
                }
            })
        
        # Parte final com bot√µes de feedback
        response_parts.append({
            'text': "\nEsta informa√ß√£o resolveu sua d√∫vida?",
            'metadata': {
                'response_part': 'confirmation',
                'buttons': [
                    {
                        'title': 'üëç Sim',
                        'payload': '/feedback_positive'
                    },
                    {
                        'title': 'üëé N√£o',
                        'payload': '/feedback_negative'
                    }
                ],
                'complete_before_next': True
            }
        })
        
        return response_parts

    def _format_faq_response(self, faq_match: Dict) -> List[Dict]:
        """Formata resposta para FAQ com bot√µes de feedback e perguntas sugeridas"""
        response_parts = [
            {
                'text': f"‚ùì **Pergunta encontrada em {faq_match['filename']}:**\n{faq_match['question']}",
                'metadata': {
                    'response_part': 'question',
                    'complete_before_next': True
                }
            },
            {
                'text': f"‚úÖ **Resposta completa:**\n{faq_match['answer']}",
                'metadata': {
                    'response_part': 'answer',
                    'complete_before_next': True
                }
            }
        ]
        
        # Adiciona informa√ß√µes extras para FAQs financeiras
        if faq_match.get('is_financial', False):
            response_parts.append({
                'text': "\nüí° Voc√™ tamb√©m pode solicitar condi√ß√µes especiais diretamente com o servi√ßo financeiro.",
                'metadata': {"type_speed": 20}
            })
        
        # Gera perguntas relacionadas
        related_questions = self._generate_related_questions(faq_match)
        if related_questions:
            questions_text = "\n\nüîç Talvez voc√™ queira saber tamb√©m:\n" + "\n".join(f"‚Ä¢ {q}" for q in related_questions)
            response_parts.append({
                'text': questions_text,
                'metadata': {
                    'type_speed': 20,
                    'suggested_questions': related_questions
                }
            })
        
        # Parte final com bot√µes de feedback
        response_parts.append({
            'text': "\nEsta informa√ß√£o resolveu sua d√∫vida?",
            'metadata': {
                'response_part': 'confirmation',
                'buttons': [
                    {
                        'title': 'üëç Sim',
                        'payload': '/feedback_positive'
                    },
                    {
                        'title': 'üëé N√£o',
                        'payload': '/feedback_negative'
                    }
                ],
                'complete_before_next': True
            }
        })
        
        return response_parts
    def _format_general_response(self, result: Dict, query: str) -> List[Dict]:
        """Formata resposta para conte√∫do geral"""
        relevant = self._find_relevant_section(result['content'], query)
        
        # Resposta especializada para quest√µes financeiras
        if self._is_financial_query(query):
            return self._format_financial_response(result)
        
        confidence = ""
        if result.get('similarity', 0) > 0.7:
            confidence = " (alta confian√ßa)"
        elif result.get('similarity', 0) > 0.5:
            confidence = " (m√©dia confian√ßa)"
        
        return [
            {
                'text': f"üìÑ **Informa√ß√£o encontrada em '{result['filename']}'{confidence}:**",
                'metadata': {"type_speed": 20}
            },
            {
                'text': f"\n\n{relevant['text']}",
                'metadata': {"type_speed": 15}
            },
            {
                'text': "\n\nPosso te ajudar com algo mais espec√≠fico sobre este conte√∫do?",
                'metadata': {"type_speed": 30, "delay": 1500}
            }
        ]


    def run(self, dispatcher, tracker, domain):
        query = tracker.latest_message.get('text', '').strip()
        self.logger.info(f"Processando consulta: '{query}'")
        
        try:
            # Busca h√≠brida - ambos os m√©todos em paralelo
            rag_results = self._semantic_search_rag(query, top_k=self.top_k)
            text_results = list(self.collection.find(
                {"$text": {"$search": query}},
                {"score": {"$meta": "textScore"}, "text_content": 1, "filename": 1}
            ).sort([("score", {"$meta": "textScore"})]).limit(3))
            
            # Combina e ordena resultados
            combined_results = []
            for result in rag_results:
                combined_results.append({
                    **result,
                    'type': 'semantic',
                    'combined_score': result.get('similarity', 0) * 0.7
                })
            
            for doc in text_results:
                combined_results.append({
                    'doc_id': doc['_id'],
                    'filename': doc.get('filename', ''),
                    'content': doc.get('text_content', ''),
                    'type': 'text',
                    'text_score': doc.get('score', 0),
                    'combined_score': doc.get('score', 0) * 0.3
                })
            
            # Remove duplicados mantendo a maior pontua√ß√£o
            unique_results = {}
            for result in combined_results:
                doc_id = str(result.get('doc_id', ''))
                if doc_id not in unique_results or result['combined_score'] > unique_results[doc_id]['combined_score']:
                    unique_results[doc_id] = result
            
            sorted_results = sorted(unique_results.values(), key=lambda x: x['combined_score'], reverse=True)
            
             # Tratamento especial para FAQs
            if self._is_faq_query(query) and sorted_results:
                faq_match = self._find_best_faq_match(query, sorted_results)
                if faq_match:
                    response_parts = self._format_faq_response(faq_match)
                    for part in response_parts:
                        if part.get('metadata', {}).get('buttons'):
                            # Se tiver bot√µes, envia como quick replies
                            dispatcher.utter_message(
                                text=part['text'],
                                buttons=part['metadata']['buttons']
                            )
                        else:
                            # Se n√£o tiver bot√µes, envia normalmente
                            dispatcher.utter_message(
                                text=part['text'],
                                metadata=part.get('metadata', {})
                            )
                    return []
            
            # Mostra o melhor resultado
            if sorted_results:
                best_result = sorted_results[0]
                
                # Se o melhor resultado tem baixa similaridade, pede confirma√ß√£o
                if best_result.get('combined_score', 0) < 0.5:
                    dispatcher.utter_message(
                        text="Encontrei algumas informa√ß√µes que podem ser relevantes, mas n√£o tenho certeza absoluta. Gostaria que eu mostrasse mesmo assim?",
                        metadata={
                            "buttons": [
                                {"title": "Sim", "payload": "/confirmar_sim"},
                                {"title": "N√£o", "payload": "/confirmar_nao"}
                            ]
                        }
                    )
                    return []
                
                for part in self._format_general_response(best_result, query):
                    dispatcher.utter_message(
                        text=part['text'],
                        metadata=part.get('metadata', {})
                    )
            else:
                dispatcher.utter_message(
                    text="N√£o encontrei informa√ß√µes sobre esse t√≥pico. Poderia reformular sua pergunta com mais detalhes?",
                    metadata={"type_speed": 30}
                )
                
        except Exception as e:
            self.logger.error(f"Erro na busca: {str(e)}", exc_info=True)
            dispatcher.utter_message(
                text="Ocorreu um erro ao processar sua solicita√ß√£o. Por favor, tente novamente mais tarde.",
                metadata={"type_speed": 30}
            )

        return []